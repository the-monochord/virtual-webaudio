Terminology:
  webaudio component: oscillators, gains, etc
  user: the code, which uses this lib
  patch: a set of instructions, which describe addition, removal or change on real elements in the browser
  scheduling: we record the timestamp of NOW and when changing a parameter of any webaudio component, we time it relative to that time

The planned route of events:
  1) the user manipulates a state object (JSON) - this might not be needed
  1) the state object will be translated to a virtual representation of creating webaudio components
  3) we diff this with the previous, internally stored virtual representation and create a patch
  4) using the patch, we do changes on a set of real webaudio nodes

What this lib doesn't do:
  we don't optimize the stuff, which was created by the user, that's not our responsibility
  we don't polyfill the webaudio api for older browsers

Questions:
  What would be and do we even need to have a root element?
    -> maybe the AudioContext
    -> what about switching to OfflineAudioContext for generating and saving sound?
  How far can we go with changing webaudio components through scheduling?
    -> When a big change is happening (e.g. new element on the multitrack editor), then we update the NOW and corrigate/remove future schedules for webaudio components
      -> the user should request a new NOW
    -> Ideally we would need to change everything through schedules, so that it can be exported into a wave
    -> Do we need to schedule everything?
      -> The sequencer can be scheduled - not our responsibility
      -> The multitrack editor's (session's) playback can be scheduled - not our responsibility
      -> nothing else needs heavy scheduling - until the user requests a new NOW, we keep scheduling
  Is there a use case for having a webaudio component detached from the context?

Next POC to work on:
  Handle time and allow recording of the monosynth, which then can be played back with the press of a button